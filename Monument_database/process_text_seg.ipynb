{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "from typing import List, Dict, Set, Literal\n",
    "import logging\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def convert_to_set_variations(terms: Set[str]) -> Set[str]:\n",
    "    \"\"\"Convert a set of underscore-separated terms into all possible variations.\n",
    "    \n",
    "    For example: 'thực_hiện' becomes {'thực_hiện', 'thực hiện'}\n",
    "    \n",
    "    Args:\n",
    "        terms: Set of terms, some with underscores\n",
    "        \n",
    "    Returns:\n",
    "        Set of terms with both underscore and space variations\n",
    "    \"\"\"\n",
    "    variations = set()\n",
    "    for term in terms:\n",
    "        variations.add(term)  # Add original form\n",
    "        if '_' in term:\n",
    "            variations.add(term.replace('_', ' '))  # Add space version\n",
    "        else:\n",
    "            variations.add(term.replace(' ', '_'))  # Add underscore version\n",
    "    return variations\n",
    "\n",
    "# Cultural terms that should always be preserved\n",
    "PRESERVE_TERMS_RAW = {\n",
    "    # Religious and ceremonial places\n",
    "    'lễ hội', 'di sản', 'văn hóa', 'tín ngưỡng', 'phong tục', 'tập quán',\n",
    "    'nghi lễ', 'cúng', 'thờ', 'đình', 'chùa', 'miếu', 'đền', 'phủ',\n",
    "    \n",
    "    # Cultural heritage terms\n",
    "    'di tích', 'danh thắng', 'di chỉ', 'cổ vật', 'hiện vật', 'bảo tàng',\n",
    "    'truyền thống', 'bản sắc', 'dân tộc', 'văn bia', 'bia đá',\n",
    "    \n",
    "    # Traditional practices\n",
    "    'hội làng', 'tế lễ', 'cầu an', 'cầu siêu', 'rước kiệu', 'tục lệ',\n",
    "    'mai táng', 'thờ cúng', 'tổ tiên', 'thần linh'\n",
    "}\n",
    "\n",
    "# Default built-in stop words\n",
    "DEFAULT_STOP_WORDS_RAW = {\n",
    "    # Basic stop words\n",
    "    'và', 'của', 'có', 'được', 'trong', 'các', 'là', 'những', 'cho', 'không',\n",
    "    'để', 'này', 'khi', 'với', 'về', 'như', 'từ', 'theo', 'tại', 'trên',\n",
    "    'đã', 'đến', 'sau', 'tới', 'vào', 'rồi', 'thì', 'mà', 'còn', 'nên',\n",
    "    \n",
    "    # Cultural heritage specific stop words\n",
    "    'ngày', 'tháng', 'năm', 'hàng', 'được', 'cùng', 'theo', 'trong', 'ngoài',\n",
    "    'trước', 'sau', 'đây', 'kia', 'ấy', 'vậy', 'nhất', 'cũng', 'lại', 'mới',\n",
    "    \n",
    "    # Temporal markers common in cultural texts\n",
    "    'xưa', 'nay', 'trước đây', 'hiện nay', 'ngày xưa', 'thời', 'khoảng',\n",
    "    'triều', 'đời', 'niên', 'kỷ', 'thế kỷ', 'thời kỳ', 'giai đoạn',\n",
    "    \n",
    "    # Ceremonial/ritual common words\n",
    "    'buổi', 'cuộc', 'dịp', 'đợt', 'lần', 'mỗi', 'việc', 'điều', 'cách',\n",
    "    'hành lễ', 'cử hành', 'tiến hành', 'thực hiện', 'tổ chức', 'diễn ra',\n",
    "    \n",
    "    # Historical document markers\n",
    "    'theo', 'căn cứ', 'dựa vào', 'qua', 'từ đó', 'do đó', 'vì thế',\n",
    "    'được biết', 'ghi chép', 'tương truyền', 'tục truyền', 'tương tự',\n",
    "    \n",
    "    # Measurement units and quantities\n",
    "    'cái', 'chiếc', 'người', 'con', 'bộ', 'đôi', 'bên', 'phía', 'nhiều',\n",
    "    'ít', 'vài', 'mấy', 'số', 'khoảng', 'chừng', 'độ', 'phần',\n",
    "    \n",
    "    # Location/spatial markers\n",
    "    'nơi', 'chỗ', 'vùng', 'miền', 'khu vực', 'địa phương', 'vị trí',\n",
    "    'hướng', 'phía', 'bên', 'trong', 'ngoài', 'trên', 'dưới',\n",
    "    \n",
    "    # Additional common words that often don't add semantic value\n",
    "    'bằng', 'ra', 'đi', 'lên', 'xuống', 'hay', 'hoặc', 'nhưng', 'tuy',\n",
    "    'dù', 'dẫu', 'mặc', 'dù', 'cho dù', 'tuy nhiên', 'song', 'nhưng mà',\n",
    "    'bởi', 'bởi vì', 'vì', 'do', 'tại vì', 'nên', 'cho nên', 'vì vậy',\n",
    "    'nếu', 'giả sử', 'giả như', 'trong trường hợp', 'khi mà', 'lúc',\n",
    "    'lúc này', 'lúc đó', 'bấy giờ', 'hồi', 'hồi đó', 'thuở', 'thuở ấy',\n",
    "    \n",
    "    # Administrative and bureaucratic terms\n",
    "    'quyết định', 'thông tư', 'nghị định', 'luật', 'điều', 'khoản', 'điểm',\n",
    "    'phụ lục', 'ban hành', 'công bố', 'thực hiện', 'áp dụng', 'có hiệu lực',\n",
    "    \n",
    "    # Redundant descriptive words\n",
    "    'rất', 'hết sức', 'vô cùng', 'cực kỳ', 'khá', 'khá là', 'tương đối',\n",
    "    'hơi', 'có phần', 'phần nào', 'đôi chút', 'chút ít', 'ít nhiều'\n",
    "}# Convert raw sets to include both underscore and space variations\n",
    "PRESERVE_TERMS = convert_to_set_variations(PRESERVE_TERMS_RAW)\n",
    "DEFAULT_STOP_WORDS = convert_to_set_variations(DEFAULT_STOP_WORDS_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_for_comparison(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison by handling spaces and underscores.\n",
    "    \"\"\"\n",
    "    # Create variations with both spaces and underscores\n",
    "    space_version = text.replace('_', ' ')\n",
    "    underscore_version = text.replace(' ', '_')\n",
    "    return f\"{space_version}|{underscore_version}\"\n",
    "\n",
    "def remove_stop_words(tokens: List[str], stop_words: Set[str], min_word_length: int = 2) -> List[str]:\n",
    "    \"\"\"Remove stop words while preserving important terms.\"\"\"\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Try to match two-word phrases first\n",
    "        two_word_match = False\n",
    "        if i < len(tokens) - 1:\n",
    "            two_words = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "            two_words_normalized = normalize_text_for_comparison(two_words)\n",
    "            \n",
    "            # Check if it's a preserve term\n",
    "            if any(term in two_words_normalized for term in PRESERVE_TERMS):\n",
    "                result.append(two_words)\n",
    "                i += 2\n",
    "                continue\n",
    "            \n",
    "            # Check if it's a stop word\n",
    "            if any(term in two_words_normalized for term in stop_words):\n",
    "                i += 2\n",
    "                continue\n",
    "        \n",
    "        # If no two-word match, process single word\n",
    "        if not two_word_match:\n",
    "            token = tokens[i]\n",
    "            token_normalized = normalize_text_for_comparison(token)\n",
    "            \n",
    "            # Enhanced filtering conditions\n",
    "            should_keep = (\n",
    "                # Not a stop word OR is a preserve term OR is long enough to be meaningful\n",
    "                (not any(term in token_normalized for term in stop_words) or\n",
    "                 any(term in token_normalized for term in PRESERVE_TERMS) or\n",
    "                 len(token) > 10) and\n",
    "                # Additional filters\n",
    "                len(token) >= min_word_length and  # Remove very short words\n",
    "                not token.isdigit() and  # Remove standalone numbers\n",
    "                not re.match(r'^[IVX]+$', token.upper())  # Remove Roman numerals\n",
    "            )\n",
    "            \n",
    "            if should_keep:\n",
    "                result.append(token)\n",
    "            i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def remove_duplicate_phrases(tokens: List[str], max_phrase_length: int = 5) -> List[str]:\n",
    "    \"\"\"Remove duplicate phrases within the token list.\"\"\"\n",
    "    result = []\n",
    "    seen_phrases = set()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        # Check for phrases of different lengths\n",
    "        for phrase_len in range(1, min(max_phrase_length + 1, len(tokens) - i + 1)):\n",
    "            phrase = ' '.join(tokens[i:i+phrase_len])\n",
    "            \n",
    "            if phrase_len == 1:  # Single word\n",
    "                if phrase not in seen_phrases:\n",
    "                    result.append(phrase)\n",
    "                    seen_phrases.add(phrase)\n",
    "                break\n",
    "            else:  # Multi-word phrase\n",
    "                if phrase in seen_phrases:\n",
    "                    # Skip this entire phrase\n",
    "                    break\n",
    "                elif phrase_len == max_phrase_length or i + phrase_len == len(tokens):\n",
    "                    # Add the longest possible unique phrase\n",
    "                    result.extend(tokens[i:i+phrase_len])\n",
    "                    seen_phrases.add(phrase)\n",
    "                    break\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(mode: Literal['builtin', 'file', 'combined'] = 'builtin', \n",
    "                  filepath: str = 'vietnamese-stopwords.txt') -> Set[str]:\n",
    "    \"\"\"Load stop words based on specified mode.\n",
    "    \n",
    "    Args:\n",
    "        mode: How to load stop words:\n",
    "            - 'builtin': Use only built-in stop words\n",
    "            - 'file': Use only file-based stop words\n",
    "            - 'combined': Combine both built-in and file-based stop words\n",
    "        filepath: Path to stop words file\n",
    "    \n",
    "    Returns:\n",
    "        Set of stop words\n",
    "    \"\"\"\n",
    "    file_stop_words = set()\n",
    "    \n",
    "    if mode in ['file', 'combined']:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                file_stop_words = {line.strip() for line in f if line.strip()}\n",
    "            logger.info(f\"Loaded {len(file_stop_words)} stop words from file\")\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"Stop words file {filepath} not found.\")\n",
    "            if mode == 'file':\n",
    "                logger.warning(\"Falling back to built-in stop words.\")\n",
    "                return DEFAULT_STOP_WORDS\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading stop words file: {str(e)}\")\n",
    "            if mode == 'file':\n",
    "                return DEFAULT_STOP_WORDS\n",
    "    \n",
    "    if mode == 'builtin':\n",
    "        logger.info(f\"Using {len(DEFAULT_STOP_WORDS)} built-in stop words\")\n",
    "        return DEFAULT_STOP_WORDS\n",
    "    elif mode == 'file':\n",
    "        return file_stop_words\n",
    "    else:  # combined\n",
    "        combined = DEFAULT_STOP_WORDS.union(file_stop_words)\n",
    "        logger.info(f\"Combined stop words: {len(combined)} total \"\n",
    "                   f\"({len(DEFAULT_STOP_WORDS)} built-in + {len(file_stop_words)} from file)\")\n",
    "        return combined\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    \"\"\"Normalize Unicode characters to canonical form.\"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def clean_text(text: str, remove_numbers: bool = True, remove_short_words: bool = True, min_word_length: int = 2) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Normalize unicode\n",
    "    text = normalize_unicode(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove numbers and dates if specified\n",
    "    if remove_numbers:\n",
    "        # Remove standalone numbers, years, dates\n",
    "        text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "        # Remove number ranges like \"12-15\", \"2020-2022\"\n",
    "        text = re.sub(r'\\d+\\s*[-–]\\s*\\d+', '', text)\n",
    "        # Remove decimal numbers\n",
    "        text = re.sub(r'\\d+[.,]\\d+', '', text)\n",
    "    \n",
    "    # Remove multiple spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep Vietnamese diacritics\n",
    "    text = re.sub(r'[^\\p{L}\\p{N}\\s]', ' ', text)\n",
    "    \n",
    "    # Remove parentheses and their contents\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Remove brackets and their contents\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    \n",
    "    # Remove multiple spaces again after all removals\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_preserve_terms(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text and preserve important multi-word terms.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    final_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = tokens[i] + '_' + tokens[i+1]\n",
    "            if bigram in PRESERVE_TERMS:\n",
    "                final_tokens.append(bigram)\n",
    "                i += 2\n",
    "                continue\n",
    "        final_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return final_tokens\n",
    "\n",
    "def filter_by_frequency(tokens: List[str], min_frequency: int = 2, max_frequency_ratio: float = 0.8) -> List[str]:\n",
    "    \"\"\"Filter tokens based on frequency within the document.\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    # Filter tokens\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        frequency = token_counts[token]\n",
    "        frequency_ratio = frequency / total_tokens\n",
    "        \n",
    "        # Keep tokens that appear at least min_frequency times but not too frequently\n",
    "        if frequency >= min_frequency and frequency_ratio <= max_frequency_ratio:\n",
    "            filtered_tokens.append(token)\n",
    "        # Always keep preserved terms regardless of frequency\n",
    "        elif any(term in token for term in PRESERVE_TERMS):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def preprocess_text(text: str, stop_words: Set[str], \n",
    "                   remove_numbers: bool = True, \n",
    "                   min_word_length: int = 2,\n",
    "                   remove_duplicates: bool = True,\n",
    "                   frequency_filter: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"Process a single text entry.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to process\n",
    "        stop_words: Set of stop words to use\n",
    "        remove_numbers: Whether to remove numbers and dates\n",
    "        min_word_length: Minimum word length to keep\n",
    "        remove_duplicates: Whether to remove duplicate phrases\n",
    "        frequency_filter: Whether to apply frequency-based filtering\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing processed versions of the text\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_text(text, remove_numbers=remove_numbers, min_word_length=min_word_length)\n",
    "    tokens = tokenize_and_preserve_terms(cleaned_text)\n",
    "    tokens = remove_stop_words(tokens, stop_words, min_word_length=min_word_length)\n",
    "    \n",
    "    # Apply additional filtering\n",
    "    if remove_duplicates:\n",
    "        tokens = remove_duplicate_phrases(tokens)\n",
    "    \n",
    "    if frequency_filter and len(tokens) > 50:  # Only apply to longer texts\n",
    "        tokens = filter_by_frequency(tokens)\n",
    "    \n",
    "    processed_text = ' '.join(tokens)\n",
    "    return {\n",
    "        'text_with_diacritics': processed_text,\n",
    "        'text_without_diacritics': unidecode(processed_text),\n",
    "        'token_count': len(tokens)\n",
    "    }\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, text_columns: List[str], \n",
    "                     mode: Literal['builtin', 'file', 'combined'] = 'builtin',\n",
    "                     remove_numbers: bool = True,\n",
    "                     min_word_length: int = 2,\n",
    "                     remove_duplicates: bool = True,\n",
    "                     frequency_filter: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Process multiple text columns in a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        text_columns: List of column names containing text to process\n",
    "        mode: Which stop words to use ('builtin', 'file', or 'combined')\n",
    "        remove_numbers: Whether to remove numbers and dates\n",
    "        min_word_length: Minimum word length to keep\n",
    "        remove_duplicates: Whether to remove duplicate phrases\n",
    "        frequency_filter: Whether to apply frequency-based filtering\n",
    "    \n",
    "    Returns:\n",
    "        Processed dataframe with new columns\n",
    "    \"\"\"\n",
    "    stop_words = load_stopwords(mode)\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    for column in text_columns:\n",
    "        if column not in df.columns:\n",
    "            logger.warning(f\"Column {column} not found in dataframe\")\n",
    "            continue\n",
    "            \n",
    "        logger.info(f\"Processing column: {column}\")\n",
    "        \n",
    "        # Process each text entry with enhanced options\n",
    "        processed_texts = [preprocess_text(text, stop_words, \n",
    "                                         remove_numbers=remove_numbers,\n",
    "                                         min_word_length=min_word_length,\n",
    "                                         remove_duplicates=remove_duplicates,\n",
    "                                         frequency_filter=frequency_filter) \n",
    "                          for text in df[column]]\n",
    "        \n",
    "        # Add new columns for processed text\n",
    "        processed_df[f\"{column}_processed\"] = [text['text_with_diacritics'] for text in processed_texts]\n",
    "        processed_df[f\"{column}_normalized\"] = [text['text_without_diacritics'] for text in processed_texts]\n",
    "        processed_df[f\"{column}_token_count\"] = [text['token_count'] for text in processed_texts]\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded CSV file with 966 rows\n",
      "INFO:__main__:Processing with enhanced_full configuration...\n",
      "INFO:__main__:Loaded 1942 stop words from file\n",
      "INFO:__main__:Combined stop words: 2066 total (229 built-in + 1942 from file)\n",
      "INFO:__main__:Processing column: title\n",
      "INFO:__main__:Processing column: content\n",
      "INFO:__main__:enhanced_full: 2149 -> 93 tokens (95.7% reduction)\n",
      "INFO:__main__:Using 229 built-in stop words\n",
      "INFO:__main__:Loaded 1942 stop words from file\n",
      "INFO:__main__:Loaded 1942 stop words from file\n",
      "INFO:__main__:Combined stop words: 2066 total (229 built-in + 1942 from file)\n",
      "INFO:__main__:Built-in stop words count: 229\n",
      "INFO:__main__:File-based stop words count: 1942\n",
      "INFO:__main__:Combined stop words count: 2066\n",
      "INFO:__main__:Unique words added from file: 1837\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv('merged.csv')\n",
    "        df = df.drop_duplicates()\n",
    "        logger.info(f\"Loaded CSV file with {len(df)} rows\")\n",
    "        \n",
    "        # Process with different stop words options\n",
    "        text_columns = ['title', 'content']\n",
    "        \n",
    "        # Test enhanced processing with different configurations\n",
    "        configs = [\n",
    "            # {'mode': 'combined', 'suffix': 'enhanced_basic', 'remove_numbers': True, 'min_word_length': 2, 'remove_duplicates': True, 'frequency_filter': False},\n",
    "            {'mode': 'combined', 'suffix': 'enhanced_full', 'remove_numbers': True, 'min_word_length': 3, 'remove_duplicates': True, 'frequency_filter': True},\n",
    "            # {'mode': 'combined', 'suffix': 'enhanced_aggressive', 'remove_numbers': True, 'min_word_length': 4, 'remove_duplicates': True, 'frequency_filter': True}\n",
    "        ]\n",
    "        \n",
    "        for config in configs:\n",
    "            logger.info(f\"Processing with {config['suffix']} configuration...\")\n",
    "            processed_df = process_dataframe(df, text_columns, \n",
    "                                           mode=config['mode'],\n",
    "                                           remove_numbers=config['remove_numbers'],\n",
    "                                           min_word_length=config['min_word_length'],\n",
    "                                           remove_duplicates=config['remove_duplicates'],\n",
    "                                           frequency_filter=config['frequency_filter'])\n",
    "            \n",
    "            output_file = f'processed_vietnamese_texts_{config[\"suffix\"]}.csv'\n",
    "            processed_df.to_csv(output_file, index=False)\n",
    "            \n",
    "            # Log token reduction statistics\n",
    "            original_tokens = len(df['content'][0].split())\n",
    "            processed_tokens = processed_df['content_token_count'][0]\n",
    "            reduction_pct = ((original_tokens - processed_tokens) / original_tokens) * 100\n",
    "            \n",
    "            logger.info(f\"{config['suffix']}: {original_tokens} -> {processed_tokens} tokens ({reduction_pct:.1f}% reduction)\")\n",
    "        \n",
    "        # Log statistics about stop words\n",
    "        builtin_words = load_stopwords('builtin')\n",
    "        file_words = load_stopwords('file')\n",
    "        combined_words = load_stopwords('combined')\n",
    "        \n",
    "        logger.info(f\"Built-in stop words count: {len(builtin_words)}\")\n",
    "        logger.info(f\"File-based stop words count: {len(file_words)}\")\n",
    "        logger.info(f\"Combined stop words count: {len(combined_words)}\")\n",
    "        logger.info(f\"Unique words added from file: {len(combined_words - builtin_words)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file: {str(e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a simple Vietnamese stemming function (basic suffix removal)\n",
    "def simple_vietnamese_stem(word: str) -> str:\n",
    "    \"\"\"Simple Vietnamese stemming by removing common suffixes.\"\"\"\n",
    "    if len(word) <= 3:\n",
    "        return word\n",
    "    \n",
    "    # Common Vietnamese suffixes to remove\n",
    "    suffixes = ['tion', 'sion', 'ness', 'ment', 'able', 'ible', 'ful', 'less', 'ly', 'ed', 'ing', 's']\n",
    "    vietnamese_suffixes = ['hóa', 'tính', 'gia', 'sư', 'viên', 'thủ', 'học']\n",
    "    \n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    # Try Vietnamese suffixes first\n",
    "    for suffix in vietnamese_suffixes:\n",
    "        if word_lower.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    \n",
    "    # Try English suffixes (for loanwords)\n",
    "    for suffix in suffixes:\n",
    "        if word_lower.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    \n",
    "    return word\n",
    "\n",
    "def apply_stemming(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Apply stemming to tokens while preserving important terms.\"\"\"\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Don't stem preserved terms\n",
    "        if any(term in token for term in PRESERVE_TERMS):\n",
    "            stemmed_tokens.append(token)\n",
    "        else:\n",
    "            stemmed_tokens.append(simple_vietnamese_stem(token))\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Test the enhanced processing\n",
    "def run_enhanced_processing_test():\n",
    "    \"\"\"Test the enhanced processing pipeline.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('merged.csv')\n",
    "        logger.info(f\"Loaded CSV file with {len(df)} rows\")\n",
    "        \n",
    "        # Test with first row only for quick comparison\n",
    "        sample_text = df['content'][0]\n",
    "        original_length = len(sample_text)\n",
    "        original_tokens = len(sample_text.split())\n",
    "        \n",
    "        logger.info(f\"Original text: {original_length} characters, {original_tokens} tokens\")\n",
    "        \n",
    "        # Test different processing levels\n",
    "        stop_words = load_stopwords('combined')\n",
    "        \n",
    "        # Basic processing (current)\n",
    "        result_basic = preprocess_text(sample_text, stop_words, \n",
    "                                     remove_numbers=False, min_word_length=1, \n",
    "                                     remove_duplicates=False, frequency_filter=False)\n",
    "        \n",
    "        # Enhanced processing\n",
    "        result_enhanced = preprocess_text(sample_text, stop_words,\n",
    "                                        remove_numbers=True, min_word_length=3,\n",
    "                                        remove_duplicates=True, frequency_filter=True)\n",
    "        \n",
    "        logger.info(f\"Basic processing: {result_basic['token_count']} tokens ({((original_tokens - result_basic['token_count']) / original_tokens * 100):.1f}% reduction)\")\n",
    "        logger.info(f\"Enhanced processing: {result_enhanced['token_count']} tokens ({((original_tokens - result_enhanced['token_count']) / original_tokens * 100):.1f}% reduction)\")\n",
    "        \n",
    "        return result_basic, result_enhanced\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in test: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison test\n",
    "basic_result, enhanced_result = run_enhanced_processing_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Dao đỏ xã Hồ Thầu huyện Hoàng Su Phì tỉnh Hà G...\n",
       "1    Lễ cầu mùa Cờ Lao đỏ xã Túng Sán huyện Hoàng S...\n",
       "2    Tây Ninh 04 tôn giáo chính Phật giáo Thiên chú...\n",
       "3    Khèn tiếng Mông gọi là Khềnh Kềnh Kỳ nhạc khí ...\n",
       "4    Dinh Thầy Thím tọa lạc giữa khu rừng dầu Bàu C...\n",
       "Name: content_processed, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv('processed_vietnamese_texts_builtin.csv')\n",
    "\n",
    "result['content_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9661"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['content_processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.read_csv('processed_vietnamese_texts_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6381"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result2['content_processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dao đỏ xã Hồ Thầu huyện Hoàng Su Phì tỉnh Hà Giang nhắc câu chuyện vị vua Bình Vương yêu thương dân Vua hộ vệ yêu Bàn Hồ long khuyển thước lông đen vằn vàng mượt nhung Cao Vương vua láng giềng đánh chiếm biên ải cảnh lầm than dân chúng Bình Vương cử binh hùng tướng trấn giữ biên ải đánh đuổi Cao Vương Trong nguy cấp Bàn Hồ hiến kế giúp Bình Vương đánh đuổi Cao Vương đem bình yên dân chúng lập công Bàn Hồ bỗng hóa thành chàng trai khôi ngô tuấn tú Bình Vương gả Tam công chúa sinh 12 6 trai 6 gái Vua Bình Vương phong vương Bàn Hồ lấy hiệu Bàn Vương ban sắc 12 Bàn Vương 12 thủy tổ tộc người Dao xấu Bàn Vương sơn dương sừng đâm ngã chết Gù hương con cháu Bàn Vương chặt Gù Hương thân trống lột da sơn dương mặt trống tế lễ Bàn Vương thế hệ con cháu Dao Dao xã Hồ Thầu tổ chức cúng tạ Bàn Vương tưởng nhớ vị sư tổ anh hùng dân tộc giáo dục truyền thống dũng cảm tự tin dịp cầu mong Bàn Vương che trở con cháu bình an may mắn Tùy ngành Dao lễ cúng nhìn chung tục cúng Bàn Vương cơ bản Dao đỏ 5 cúng ngành Dao đỏ 9 cúng dòng họ 12 cúng Dao áo dài cúng Bàn Vương lễ cúng cúng Bàn Vương lễ chay lễ cấp sắc cúng Tổ tiên Hàng 15 tháng Mười 30 tháng Chạp Âm lịch con cháu Dao đỏ thay phiên Chủ lễ cúng tổ Bàn Vương Người Dao đỏ xã Hồ Thầu thực hành nghi lễ phổ biến 2 hình thức Đàng Ton lễ cúng quy mô dòng họ lễ cúng gia đình Nghi lễ Đàng Ton 12 dòng họ tham gia dòng họ cử 1 nam 1 nữ đồng trinh dâng hương lễ Bàn Vương Các dòng họ luân phiên Chủ lễ Trường hợp thiên tai mất mùa kinh tế gia đình đồng bào lễ cúng khất Bàn Vương mùa màng bội thu lễ cúng chính thức Thông thường gia đình dòng họ nuôi lợn nái đẻ lợn đực coi lợn Bàn Vương gọi là lợn thần cử Chủ lễ trách nhiệm chăm sóc lợn thần nuôi chuồng mổ cúng dâng Bàn Vương lễ tổ chức dòng họ coi Bàn Vương ông tổ tập hợp tổ chức cúng truyền thống diễn dài Lễ cúng Bàn Vương gia đình tiến hành kết hợp lễ cúng gia đình lễ sắc lễ cúng cơm cúng vía lúa chay nghi lễ mời thầy cúng đàn ông chủ lễ cấp sắc khả năng quy mô gia đình Dao đỏ tổ chức cúng Bàn Vương gia đình may ốm đau dịch bệnh mất mùa kiểu giải hạn song hình thức tổ chức lễ cúng Bàn Vương lễ vật 01 lợn sạch lòng tiết 01 gà trống 12 cặp bánh dày gói lá chuối buộc cặp 01 túm gạo buộc vải trắng sài chiên Giấy bản cúng bình thường 7 siên chìn shún thầy cúng cắt hình đục dập hình khuôn chữ 02 chum rượu 01 cúng Bàn Vương 01 cúng Tổ tiên vị Thần Đàn cúng tế Bàn Vương lập gỗ gian cạnh bàn thờ tổ tiên đàn tế trang trí giấy màu tranh thờ vải đồ vật trang trí kết tre nứa cúng Bàn Vương thầy cúng gia đình dòng họ tham gia lễ kiêng 7 gia chủ mời dự lễ vợ chồng cách ly tranh gia chủ đun tắm sạch sẽ xong cất giữ tranh thờ coi chốn linh thiêng dự lễ ăn chay quở mắng chê bai Lễ cúng Bàn Vương tiến hành trình tự Gia chủ gia đình dòng họ cử Chủ lễ đi mời thầy cúng gia đình lễ Thầy cúng gọi là Thầy Dao Đỏ gọi vìn nhủng sai cúng lợn thần Bàn Vương đời Tổ tiên thầy cúng hai Dao Đỏ gọi là sai pành piê Dao Tiền gọi diang gòa cúng cầu khấn sức khỏe Thần lúa Thần chăn nuôi thầy cúng cúng lễ vật vị Thần Tổ tiên thầy cúng mời 3 thầy phụ lễ bàn bạc trao đổi thầy ấn định tổ chức lễ cúng Bàn Vương Thầy gia đình nghi thức Cắt giấy dập chữ đất thầy cúng trình báo bàn thờ Tổ tiên lễ vật phong tục lễ lễ thầy cúng gia chủ lễ lập đàn cúng Gian treo tranh Tam Thanh 2 bàn thờ Tổ tiên treo hai tranh Tam Thanh thầy cúng thầy cúng làm phép tẩy uế vẩy nước phép khắp làm phép trấn an dán bùa phép viết chữ Nôm Dao giấy quanh khấn mời ma Bàn Vương Tổ tiên Thần âm binh thầy dự lễ bàn thờ nghi lễ dâng Tổ tiên bữa cơm chay thầy cúng thay mặt gia chủ thỉnh mời 3 vị Tam thanh Bàn Vương Tổ tiên Thổ địa dự lễ đồ lễ thầy cúng mời lễ lập đàn khấn Bàn Vương gia tiên thần thánh chứng giám buổi lễ lợn cúng thần đàn cúng cúng gia tiên Bàn Vương nghi thức dâng hương đại diện 12 dòng họ Dao 12 cặp nam nữ đồng trinh lần lượt dâng hương dâng lễ vật đứng thầy vái lạy đón chào ma Tổ tiên ma Bàn Vương Thần Thánh đàn cúng lễ vật lợn lợn thần mổ bát gạo bát chai rượu tiền âm phủ 5 chén 5 đôi đũa Sáu thầy cúng hai ghế dài kê song song đối diện hai đàn cúng Thầy trịnh trọng khấn tế lễ dâng lợn thần Bàn Vương cầu mong Bàn Vương phù hộ con cháu gia đình Chủ lễ gia tộc người Dao thầy phụ lần lượt đọc cúng khai thiên lập địa sự tích nạn hồng thủy chuyển cư gian truân Dao Tiếp thầy đọc tích truyện công ơn sinh Dao Bàn Vương phụ lễ bày tiếp gạo rượu bánh bàn cúng ban thờ Tổ tiên Thầy cúng tế lễ Bàn Vương bậc Tổ tiên vị Thần Thánh dòng họ gia chủ điệu múa mường tượng hóa múa bắt rùa múa chuông 6 áo thầy cúng lần lượt tái hiện điệu múa cổ nghi thức lễ cúng Bàn Vương cúng tế Bàn Vương Tổ tiên vị Thần Thánh gia chủ 12 dòng họ Dao thầy cúng hóa tiền vàng hát tiễn đưa vái chào từ biệt Bàn Vương bậc Tổ tiên vị Thần Thánh cõi dương gian lễ cúng Bàn Vương trải 7 1 mời Bàn Vương thần linh dự lễ 2 thổi tù báo hiệu Bàn Vương lễ 3 dâng hương dâng lễ tạ ơn Bàn Vương 4 múa gọi thiên linh thổ địa nghênh đón Bàn Vương sơ lược tiểu sử vóc dáng Bàn Vương ra đời 5 múa gậy thể hiện Bàn Vương thần linh thổ địa đàn lễ vật 6 ca hát vui Bàn Vương đàn cúng 7 hóa vàng tiễn Bàn Vương trời cúng Bàn vương dân làng quây quần dự bữa cơm cộng cảm thầy cúng thổi tù mời gọi Bàn Vương vị Thần linh hưởng lễ chứng minh lễ vật thành 12 dòng họ dâng thầy rung chuông tập hợp cúng múa chuông múa gậy thể hiện sức mạnh đoàn kết đánh đuổi ngoại xâm nghi lễ xua đuổi tà ma xấu ác đem bình yên may mắn con cháu nghi lễ cộng đồng Dao tổ chức trò chơi vật chày trò chơi màu sắc huyền bí đem kịch tính tiếng cười tham gia vật chày trò chơi dân gian thể hiện sức mạnh kết hợp khéo léo bí ẩn làm phép vật thầy cúng làm phép Thần linh giúp đỡ đông tham gia ấn nổi đầu chày chạm đất thể hiện bản làng vị thần phù hộ nâng đỡ Tục thờ cúng Bàn Vương biểu tượng thống nhất nguồn gốc Dao thể hiện biết ơn Tổ tiên sinh Lễ cúng Bàn Vương cầu mưa thuận gió hòa hoa màu tươi con cháu Dao đời đời ấm no hạnh phúc Thực hành nghi lễ thỏa mãn nhu cầu tâm linh người dân giúp gắn kết cộng đồng bảo lưu văn hóa truyền thống Lễ cúng Bàn Vương dịp gặp gỡ giao lưu trao đổi tình cảm sinh hoạt cộng đồng đoàn kết gắn bó môi trường giáo dục truyền thống thế hệ trẻ trao truyền cúng hình thức nghi lễ thế hệ kế cận góp phần bảo tồn phát huy bản sắc văn hóa Dao đỏ tiêu biểu Lễ cúng Bàn Vương Dao Đỏ Bộ trưởng Văn hóa Thể thao Du lịch Danh mục di sản văn hóa phi vật thể quốc gia Quyết định 779 QĐ BVHTTDL 04 4 2022 Dương Anh Hồ sơ tư liệu Cục Di sản văn hóa'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2['content_processed'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
