{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('processed_vietnamese_texts_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns:\n",
      "['url', 'title', 'content', 'title_processed', 'title_normalized', 'title_token_count', 'content_processed', 'content_normalized', 'content_token_count']\n",
      "\n",
      "Dataset shape: (1163, 9)\n",
      "\n",
      "First row sample:\n",
      "                                                 url  \\\n",
      "0  https://dsvh.gov.vn/le-cung-ban-vuong-cua-nguo...   \n",
      "1  https://dsvh.gov.vn/le-cau-mua-cua-nguoi-co-la...   \n",
      "2  https://dsvh.gov.vn/nghe-thuat-che-bien-mon-an...   \n",
      "3  https://dsvh.gov.vn/nghe-thuat-mua-khen-cua-ng...   \n",
      "4     https://dsvh.gov.vn/le-hoi-dinh-thay-thim-3454   \n",
      "\n",
      "                                        title  \\\n",
      "0          Lá»… cÃºng BÃ n vÆ°Æ¡ng cá»§a ngÆ°á»i Dao Ä‘á»   \n",
      "1              Lá»… cáº§u mÃ¹a cá»§a ngÆ°á»i Cá» Lao Ä‘á»   \n",
      "2  Nghá»‡ thuáº­t cháº¿ biáº¿n mÃ³n Äƒn chay á»Ÿ TÃ¢y Ninh   \n",
      "3          Nghá»‡ thuáº­t mÃºa KhÃ¨n cá»§a ngÆ°á»i MÃ´ng   \n",
      "4                       Lá»… há»™i dinh Tháº§y ThÃ­m   \n",
      "\n",
      "                                             content        title_processed  \\\n",
      "0  NgÆ°á»i Dao Ä‘á» (xÃ£ Há»“ Tháº§u, huyá»‡n HoÃ ng Su PhÃ¬, ...             Lá»… cÃºng Ä‘á»   \n",
      "1  Lá»… cáº§u mÃ¹a cá»§a ngÆ°á»i Cá» Lao Ä‘á» (xÃ£ TÃºng SÃ¡n, h...              Lá»… cáº§u Ä‘á»   \n",
      "2  TÃ¢y Ninh cÃ³ 04 tÃ´n giÃ¡o chÃ­nh: Pháº­t giÃ¡o, ThiÃª...    Nghá»‡ thuáº­t cháº¿ biáº¿n   \n",
      "3  KhÃ¨n (tiáº¿ng MÃ´ng gá»i lÃ  Khá»nh, Ká»nh, Ká»³) lÃ  nh...             Nghá»‡ thuáº­t   \n",
      "4  Dinh Tháº§y ThÃ­m tá»a láº¡c giá»¯a khu rá»«ng dáº§u BÃ u C...  Lá»… há»™i dinh Tháº§y ThÃ­m   \n",
      "\n",
      "        title_normalized  title_token_count  \\\n",
      "0             Le cung do                  2   \n",
      "1              Le cau do                  3   \n",
      "2    Nghe thuat che bien                  2   \n",
      "3             Nghe thuat                  1   \n",
      "4  Le hoi dinh Thay Thim                  3   \n",
      "\n",
      "                                   content_processed  \\\n",
      "0  Ä‘á» PhÃ¬ há»™ vá»‡ thá»§y thÃ¢n trá»‘ng lá»™t trá»‘ng táº¿ lá»… c...   \n",
      "1  Lá»… cáº§u Ä‘á» PhÃ¬ ra miáº¿u thá» HoÃ n Váº§n ThÃ¹ng vá»‹ mÃ¹...   \n",
      "2  chÃ¹a tá»‹nh xÃ¡ CÃ¡c chÃ¹a cÃºng chay quanh nÄƒm mÃ¹ng...   \n",
      "3  KhÃ¨n CÃ¹ng vá»›i tháº§y cÃºng kÃ­nh trá»ng lá»— gá»t thÃ¢n...   \n",
      "4  Dinh Tháº§y thÃ´n Tam TÃ¢n xÃ£ TÃ¢n Tiáº¿n thá»‹ xÃ£ La G...   \n",
      "\n",
      "                                  content_normalized  content_token_count  \n",
      "0  do Phi ho ve thuy than trong lot trong te le c...                  197  \n",
      "1  Le cau do Phi ra mieu tho Hoan Van Thung vi mu...                  152  \n",
      "2  chua tinh xa Cac chua cung chay quanh nam mung...                  179  \n",
      "3  Khen Cung voi thay cung kinh trong lo got than...                  184  \n",
      "4  Dinh Thay thon Tam Tan xa Tan Tien thi xa La G...                  205  \n"
     ]
    }
   ],
   "source": [
    "# Let's first examine the structure with just 1 row as requested\n",
    "print(\"Dataset columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nFirst row sample:\")\n",
    "sample_row = df.iloc[0:5]\n",
    "print(sample_row)\n",
    "\n",
    "# Save just 1 row for reference as requested\n",
    "# df[0:1].to_csv('sample_ref.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Vietnamese NER with underthesea - handling 4-tuple format\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_entities_fixed(text):\n",
    "    \"\"\"Extract entities using underthesea NER - fixed for 4-tuple format\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        ner_results = ner(text[:800])  # Slightly longer text for better context\n",
    "        \n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_type = None\n",
    "        \n",
    "        for result in ner_results:\n",
    "            # Handle the 4-tuple format: (word, pos_tag, chunk_tag, ner_tag)\n",
    "            if isinstance(result, tuple):\n",
    "                if len(result) == 4:\n",
    "                    word, pos_tag, chunk_tag, ner_tag = result\n",
    "                elif len(result) == 3:\n",
    "                    word, pos_tag, ner_tag = result\n",
    "                elif len(result) == 2:\n",
    "                    word, ner_tag = result\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Process NER tags (B-*, I-*, O)\n",
    "            if ner_tag.startswith('B-'):  # Beginning of entity\n",
    "                # Save previous entity if exists\n",
    "                if current_entity and current_type:\n",
    "                    entity_text = ' '.join(current_entity).strip()\n",
    "                    if len(entity_text) > 1:  # Skip single characters\n",
    "                        entities.append({\n",
    "                            'text': entity_text,\n",
    "                            'type': current_type\n",
    "                        })\n",
    "                \n",
    "                # Start new entity\n",
    "                current_entity = [word]\n",
    "                current_type = ner_tag[2:]  # Remove 'B-' prefix\n",
    "                \n",
    "            elif ner_tag.startswith('I-') and current_type == ner_tag[2:]:\n",
    "                # Continue current entity\n",
    "                current_entity.append(word)\n",
    "                \n",
    "            else:\n",
    "                # End current entity (O tag or different entity type)\n",
    "                if current_entity and current_type:\n",
    "                    entity_text = ' '.join(current_entity).strip()\n",
    "                    if len(entity_text) > 1:\n",
    "                        entities.append({\n",
    "                            'text': entity_text,\n",
    "                            'type': current_type\n",
    "                        })\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        \n",
    "        # Don't forget the last entity\n",
    "        if current_entity and current_type:\n",
    "            entity_text = ' '.join(current_entity).strip()\n",
    "            if len(entity_text) > 1:\n",
    "                entities.append({\n",
    "                    'text': entity_text,\n",
    "                    'type': current_type\n",
    "                })\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in NER: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_vietnamese_heritage_texts(df, num_docs=10):\n",
    "    \"\"\"Process Vietnamese heritage texts with entity extraction\"\"\"\n",
    "    print(f\"ðŸ›ï¸ Processing {num_docs} Vietnamese heritage documents...\")\n",
    "    \n",
    "    all_entities = defaultdict(list)\n",
    "    doc_entities = []\n",
    "    \n",
    "    for idx, row in df.head(num_docs).iterrows():\n",
    "        title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "        content = str(row['content']) if pd.notna(row['content']) else \"\"\n",
    "        full_text = f\"{title} {content}\"\n",
    "        \n",
    "        print(f\"\\nðŸ“œ Document {idx+1}: {title[:60]}...\")\n",
    "        \n",
    "        # Extract all entities\n",
    "        entities = extract_entities_fixed(full_text)\n",
    "        \n",
    "        # Categorize entities\n",
    "        locations = [e for e in entities if e['type'] in ['LOC', 'LOCATION']]\n",
    "        people = [e for e in entities if e['type'] in ['PER', 'PERSON']]\n",
    "        organizations = [e for e in entities if e['type'] in ['ORG', 'ORGANIZATION']]\n",
    "        \n",
    "        doc_info = {\n",
    "            'doc_index': idx,\n",
    "            'title': title,\n",
    "            'total_entities': len(entities),\n",
    "            'locations': locations,\n",
    "            'people': people,\n",
    "            'organizations': organizations,\n",
    "            'all_entities': entities\n",
    "        }\n",
    "        doc_entities.append(doc_info)\n",
    "        \n",
    "        # Add to global collection\n",
    "        for entity in entities:\n",
    "            all_entities[entity['type']].append({\n",
    "                'text': entity['text'],\n",
    "                'doc_title': title,\n",
    "                'doc_index': idx\n",
    "            })\n",
    "        \n",
    "        # Print quick summary for this doc\n",
    "        print(f\"   âœ… Found {len(entities)} entities:\")\n",
    "        if len(entities) > 0:\n",
    "            entity_types = list(set([e['type'] for e in entities]))\n",
    "            for et in entity_types:\n",
    "                count = len([e for e in entities if e['type'] == et])\n",
    "                examples = [e['text'] for e in entities if e['type'] == et][:3]\n",
    "                print(f\"      {et}: {count} entities - {', '.join(examples)}\")\n",
    "        else:\n",
    "            print(\"      No entities found\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nðŸŽ¯ HERITAGE ENTITY SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    for entity_type, entities in all_entities.items():\n",
    "        unique_entities = list(set([e['text'] for e in entities]))\n",
    "        print(f\"{entity_type}: {len(entities)} total, {len(unique_entities)} unique\")\n",
    "        \n",
    "        # Show top examples\n",
    "        for i, entity in enumerate(unique_entities[:5]):\n",
    "            count = len([e for e in entities if e['text'] == entity])\n",
    "            print(f\"  â€¢ {entity} (appears {count} times)\")\n",
    "        if len(unique_entities) > 5:\n",
    "            print(f\"  ... and {len(unique_entities)-5} more unique entities\")\n",
    "        print()\n",
    "    \n",
    "    return all_entities, doc_entities\n",
    "\n",
    "# Import NER function\n",
    "from underthesea import ner\n",
    "\n",
    "# Test the fixed version\n",
    "print(\"ðŸš€ Testing FIXED underthesea NER on Vietnamese heritage texts...\")\n",
    "entities, doc_details = process_vietnamese_heritage_texts(df, num_docs=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
